version: "3"

services:

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    restart: always
    container_name: zookeeper
    hostname: zookeeper
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 2181 || exit1"]
      interval: 5s
      timeout: 5s
      retries: 60
    ports:
      - "2181:2181"
    networks:
      - default
    environment:
      ZOO_MY_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181

  broker:
    container_name: broker
    image: confluentinc/cp-kafka:latest
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit1"]
      interval: 5s
      timeout: 5s
      retries: 60
    ports:
      - "9092:9092"
      #- "29092:29092"
    networks:
      - default
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"

      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_MESSAGE_MAX_BYTES: 2000000

  init-kafka:
      image: confluentinc/cp-kafka:latest
      container_name: init-kafka
      #image: confluentinc/cp-kafka:6.1.1
      depends_on:
        - broker
      entrypoint: [ '/bin/sh', '-c' ]
      command: |
        "
        # blocks until kafka is reachable
        kafka-topics --bootstrap-server broker:29092 --list
  
        echo -e 'Creating kafka topics'
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $ICE_SHEET_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $ICE_SHEET_DIFF_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $CARIBOU_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $CONTROL_TOPIC --replication-factor 1 --partitions 1
  
        echo -e 'Successfully created the following topics:'
        kafka-topics --bootstrap-server broker:29092 --list
        "
      networks:
        - default

          #kafka-schema-1:  # for avro and json schemata
          #  image: 'confluentinc/cp-schema-registry:latest'
          #  container_name: 'kafka-schema-1'
          #  hostname: 'kafka-schema-1'
          #  healthcheck:
          #    test: ["CMD-SHELL", "nc -z kafka-sr1 8081 || exit 1" ]
          #    interval: 5s
          #    timeout: 5s
          #    retries: 60
          #  networks:
          #    - default
          #  ports:
          #    - '8081:8081'
          #  environment:
          #    SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:9092'
          #          #SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'localhost:9092'
          #    SCHEMA_REGISTRY_HOST_NAME: kafka-schema-1
          #    SCHEMA_REGISTRY_LISTENERS: 'http://kafka-schema-1:8081'
          #  depends_on:
          #    - zookeeper
   
  #connect_1:
  #  image: 'confluentinc/cp-kafka-connect:latest'
  #  container_name: 'connect_1'
  #  healthcheck:
  #    test: ["CMD-SHELL", "nc -z localhost 8083 || exit1"]
  #    interval: 5s
  #    timeout: 5s
  #    retries: 60
  #  build:
  #    context: .
  #    dockerfile: connectDockerfile
  #  depends_on:
  #    - zookeeper
  #    - broker
  #  ports:
  #    - "8083:8083"
  #  networks:
  #    - default
  #  volumes:
  #          #- ./vol-kafka-connect-jar:/etc/kafka-connect/jars
  #    - ./vol-kafka-connect-conf:/etc/kafka-connect/connectors
  #  environment:
  #    CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
  #    CONNECT_REST_PORT: 8083
  #    CONNECT_GROUP_ID: cassandraConnect
  #    CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1  # changing default from 3 because we only have one broker
  #    CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1  # changing default from 3 because we only have one broker
  #    CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1  # changing default from 3 because we only have one broker
  #    CONNECT_CONFIG_STORAGE_TOPIC: cassandraconnect-config
  #    CONNECT_OFFSET_STORAGE_TOPIC: cassandraconnect-offset
  #    CONNECT_STATUS_STORAGE_TOPIC: cassandraconnect-status
  #    #CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
  #    CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
  #    CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter

  #    # trigger warnings 
  #    CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
  #    CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
  #    CONNECT_KEY_CONVERTER_SCHHEMAS_ENABLE: "false"
  #    CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
  #    # end of warnings 
  #    
  #    CONNECT_REST_ADVERTISED_HOST_NAME: "connect-hostname"
  #    CONNECT_REST_ADVERTISED_HOST_NAME: "connect_1"
  #    CONNECT_PLUGIN_PATH: "/etc/kafka-connect/jars"

  #    CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'

  connect:
    image: datastax-connect:latest
    hostname: datastax-connect
    container_name: datastax-connect
    depends_on:
      - zookeeper
      - broker
    ports:
      - "8083:8083"
    networks:
      - default
    volumes:
      - ./vol-kafka-connect-conf:/etc/kafka-connect/connectors
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 8083 || exit1"]
      interval: 5s
      timeout: 5s
      retries: 60
    build:
      context: .
      dockerfile: test_connectDockerfile
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: datastax-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: datastax-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'


  cassandra-1:
    image: 'cassandra:latest'
    container_name: 'cassandra-1'
    hostname: 'cassandra-1'
    build:
      context: .
      dockerfile: Dockerfile-cassandra
    healthcheck:
      test: ["CMD-SHELL", "cqlsh", "-e", "describe keyspaces" ]
      interval: 5s
      timeout: 5s
      retries: 60
    networks:
      - default
    ports:
      - "9042:9042"
    environment:  # should other containers be dependent on cassandra to wait until everything is running? possibly less efficient...
      - CASSANDRA_CLUSTER_NAME=icesheets_cluster
      - CASSANDRA_SEEDS=cassandra-1
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_RACK=rack1
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_NUM_TOKENS=128
      - HEAP_NEWSIZE=128M
      - MAX_HEAP_SIZE=2048M


        #kafka_manager:  # as num of clusters increase, it may be needed
          #  image: hlebalbau/kafka-manager:stable
          #  container_name: kakfa-manager
          #  restart: always
          #  ports:
          #    - 9000:9000
          #  networks:
          #    - default
          #  environment:
          #    ZK_HOSTS: "zookeeper:2181"
          #    APPLICATION_SECRET: "random-secret"
          #    KAFKA_MANAGER_AUTH_ENABLED: "true"
          #    KAFKA_MANAGER_USERNAME: admin
          #    KAFKA_MANAGER_PASSWORD: smellycat
          #  command: -Dpidfile.path=/dev/null


networks:
  default:
    external:
      name: kafka-network
