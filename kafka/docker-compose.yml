version: "3"

services:

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    restart: always
    container_name: zookeeper
    hostname: zookeeper
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 2181 || exit1"]
      interval: 5s
      timeout: 5s
      retries: 60
    ports:
      - "2181:2181"
    networks:
      - default
    environment:
      ZOO_MY_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181

  broker:
    container_name: broker
    image: confluentinc/cp-kafka:latest
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit1"]
      interval: 5s
      timeout: 5s
      retries: 60
    ports:
      - "9092:9092"
      #- "29092:29092"
    networks:
      - default
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"

      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_MESSAGE_MAX_BYTES: 2000000

  init-kafka:
      image: confluentinc/cp-kafka:latest
      container_name: init-kafka
      #image: confluentinc/cp-kafka:6.1.1
      depends_on:
        - broker
      entrypoint: [ '/bin/sh', '-c' ]
      command: |
        "
        # blocks until kafka is reachable
        kafka-topics --bootstrap-server broker:29092 --list
  
        echo -e 'Creating kafka topics'
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $ICE_SHEET_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $ICE_SHEET_DIFF_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $ICE_SHEET_PRED_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $CARIBOU_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $CARIBOU_SPLIT_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $OCEAN_HEAT_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $GLOBAL_TEMP_TOPIC --replication-factor 1 --partitions 1
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic $CONTROL_TOPIC --replication-factor 1 --partitions 1
  
        echo -e 'Successfully created the following topics:'
        kafka-topics --bootstrap-server broker:29092 --list
        "
      networks:
        - default

  cassandra-1:
    image: 'cassandra:latest'
    container_name: 'cassandra-1'
    hostname: 'cassandra-1'
    build:
      context: .
      dockerfile: Dockerfile-cassandra
    healthcheck:
      test: ["CMD-SHELL", "cqlsh", "-e", "describe keyspaces" ]
      interval: 5s
      timeout: 5s
      retries: 60
    networks:
      - default
    ports:
      - "9042:9042"
    environment:  # should other containers be dependent on cassandra to wait until everything is running? possibly less efficient...
      - CASSANDRA_CLUSTER_NAME=icesheets_cluster
      - CASSANDRA_SEEDS=cassandra-1
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_RACK=rack1
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_NUM_TOKENS=128
      - HEAP_NEWSIZE=128M
      - MAX_HEAP_SIZE=2048M


        #kafka_manager:  # as num of clusters increase, it may be needed
          #  image: hlebalbau/kafka-manager:stable
          #  container_name: kakfa-manager
          #  restart: always
          #  ports:
          #    - 9000:9000
          #  networks:
          #    - default
          #  environment:
          #    ZK_HOSTS: "zookeeper:2181"
          #    APPLICATION_SECRET: "random-secret"
          #    KAFKA_MANAGER_AUTH_ENABLED: "true"
          #    KAFKA_MANAGER_USERNAME: admin
          #    KAFKA_MANAGER_PASSWORD: smellycat
          #  command: -Dpidfile.path=/dev/null


networks:
  default:
    external:
      name: kafka-network
